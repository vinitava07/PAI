import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2  # Usando MobileNetV2 ao inv√©s de InceptionV3
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input  # Pr√©-processamento espec√≠fico do MobileNet
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, Tuple, List, Optional


class MobileNetALNClassifier:
    """
    Pipeline completa para classifica√ß√£o de patches de c√¢ncer de mama usando MobileNetV2 pr√©-treinado
    Adaptado do InceptionV3ALNClassifier para usar MobileNetV2
    """
    
    def __init__(self, patches_dir, clinical_data_path):
        """
        Inicializa o classificador MobileNet
        
        Args:
            patches_dir: Diret√≥rio contendo patches organizados por paciente
            clinical_data_path: Caminho para arquivo CSV com dados cl√≠nicos
        """
        self.patches_dir = Path(patches_dir)
        self.clinical_data_path = clinical_data_path
        self.clinical_data = None
        self.model = None
        self.class_weights = None
        self.input_size = (224, 224)  # Tamanho padr√£o MobileNetV2 (menor que Inception)
        self.num_classes = 3
        self.class_names = ['N0', 'N+(1-2)', 'N+(>2)']
        
        # Carrega dados cl√≠nicos
        self._load_clinical_data()
        
    def _load_clinical_data(self):
        """Carrega e processa dados cl√≠nicos"""
        print("Carregando dados cl√≠nicos...")
        self.clinical_data = pd.read_csv(self.clinical_data_path)
        
        # Mapeia ALN status para c√≥digos num√©ricos
        aln_mapping = {
            'N0': 0,
            'N+(1-2)': 1, 
            'N+(>2)': 2
        }
        self.clinical_data['ALN_encoded'] = self.clinical_data['ALN status'].map(aln_mapping)
        
        print(f"Dados carregados: {len(self.clinical_data)} pacientes")
        print(f"Distribui√ß√£o das classes: {self.clinical_data['ALN status'].value_counts().to_dict()}")

    def prepare_dataset(self, max_patches_per_patient: Optional[int] = None,
                        test_size: float = 0.2,
                        val_size: float = 0.25) -> Dict[str, pd.DataFrame]:
        """
        Prepara dataset com divis√£o correta por paciente
        Reutiliza a l√≥gica do Inception mas com input_size espec√≠fico do MobileNet

        Args:
            max_patches_per_patient: Limite de patches por paciente (None = todos)
            test_size: Propor√ß√£o para teste (padr√£o 20%)
            val_size: Propor√ß√£o para valida√ß√£o do treino

        Returns:
            Dicion√°rio com DataFrames para treino, valida√ß√£o e teste
        """
        print(f"\nüìÅ Preparando dataset de patches para MobileNetV2...")
        print(f"  Max patches/paciente: {max_patches_per_patient or 'Todos'}")
        print(f"  Tamanho de entrada: {self.input_size}")

        # Coleta informa√ß√µes dos patches
        patch_data = []
        patient_patch_counts = {}

        for _, patient_row in self.clinical_data.iterrows():
            patient_id = str(patient_row['Patient ID'])
            aln_class = int(patient_row['ALN_encoded'])
            patient_dir = self.patches_dir / patient_id

            if not patient_dir.exists():
                print(f"‚ö†Ô∏è Diret√≥rio n√£o encontrado: {patient_dir}")
                continue

            # Lista patches do paciente
            patch_files = sorted(list(patient_dir.glob("*.jpg")))

            if not patch_files:
                print(f"‚ö†Ô∏è Nenhum patch encontrado para paciente {patient_id}")
                continue

            # Limita n√∫mero de patches se especificado
            if max_patches_per_patient and len(patch_files) > max_patches_per_patient:
                # Seleciona aleatoriamente para evitar vi√©s
                np.random.seed(42)  # Reprodutibilidade
                indices = np.random.choice(len(patch_files), max_patches_per_patient, replace=False)
                patch_files = [patch_files[i] for i in sorted(indices)]

            patient_patch_counts[patient_id] = len(patch_files)

            # Adiciona patches ao dataset
            for patch_file in patch_files:
                patch_data.append({
                    'patch_path': str(patch_file),
                    'patient_id': patient_id,
                    'class': aln_class,
                    'class_name': self.class_names[aln_class]
                })

        if not patch_data:
            raise ValueError("‚ùå Nenhum patch foi encontrado!")

        # Converte para DataFrame
        df_patches = pd.DataFrame(patch_data)

        print(f"\nüìä Estat√≠sticas dos patches:")
        print(f"  Total de patches: {len(df_patches)}")
        print(f"  Pacientes com patches: {len(patient_patch_counts)}")
        print(f"  M√©dia patches/paciente: {np.mean(list(patient_patch_counts.values())):.1f}")

        # Distribui√ß√£o por classe
        print("\nüìä Distribui√ß√£o de patches por classe:")
        for class_name, count in df_patches['class_name'].value_counts().items():
            percentage = (count / len(df_patches)) * 100
            print(f"  {class_name}: {count} patches ({percentage:.1f}%)")

        # DIVIS√ÉO POR PACIENTE (n√£o por patch!)
        print(f"\nüîÑ Dividindo dataset por paciente...")

        # Lista √∫nica de pacientes e suas classes
        unique_patients = df_patches['patient_id'].unique()
        patient_classes = []

        for pid in unique_patients:
            patient_class = df_patches[df_patches['patient_id'] == pid]['class'].iloc[0]
            patient_classes.append(patient_class)

        # Primeira divis√£o: 80% treino+val, 20% teste
        patients_train_val, patients_test, y_train_val, y_test = train_test_split(
            unique_patients,
            patient_classes,
            test_size=test_size,
            random_state=42,
            stratify=patient_classes
        )

        # Segunda divis√£o: separa valida√ß√£o do treino
        patients_train, patients_val, y_train, y_val = train_test_split(
            patients_train_val,
            y_train_val,
            train_size=0.75,
            test_size=0.25,
            random_state=43,
            stratify=y_train_val
        )

        # Cria DataFrames finais
        train_patches = df_patches[df_patches['patient_id'].isin(patients_train)]
        val_patches = df_patches[df_patches['patient_id'].isin(patients_val)]
        test_patches = df_patches[df_patches['patient_id'].isin(patients_test)]

        # Embaralha patches dentro de cada conjunto
        train_patches = train_patches.sample(frac=1, random_state=42).reset_index(drop=True)
        val_patches = val_patches.sample(frac=1, random_state=42).reset_index(drop=True)
        test_patches = test_patches.sample(frac=1, random_state=42).reset_index(drop=True)

        # Estat√≠sticas finais
        print(f"\n‚úÖ Divis√£o conclu√≠da:")
        print(f"  Pacientes treino: {len(patients_train)} ({len(train_patches)} patches)")
        print(f"  Pacientes valida√ß√£o: {len(patients_val)} ({len(val_patches)} patches)")
        print(f"  Pacientes teste: {len(patients_test)} ({len(test_patches)} patches)")

        # Verifica distribui√ß√£o de classes
        print("\nüìä Distribui√ß√£o de classes por conjunto:")
        for split_name, split_df in [('Treino', train_patches),
                                     ('Valida√ß√£o', val_patches),
                                     ('Teste', test_patches)]:
            dist = split_df['class_name'].value_counts()
            print(f"\n  {split_name}:")
            for class_name in self.class_names:
                count = dist.get(class_name, 0)
                percentage = (count / len(split_df)) * 100 if len(split_df) > 0 else 0
                print(f"    {class_name}: {count} ({percentage:.1f}%)")

        return {
            'train': train_patches,
            'val': val_patches,
            'test': test_patches,
            'all': df_patches
        }

    def preprocess_image(self, image_path):
        """
        Pr√©-processa uma imagem para o modelo MobileNetV2
        
        Args:
            image_path: Caminho da imagem
            
        Returns:
            Array numpy da imagem processada
        """
        # Carrega imagem com tamanho espec√≠fico do MobileNet
        img = load_img(image_path, target_size=self.input_size)
        img_array = img_to_array(img)
        
        # Adiciona dimens√£o do batch
        img_array = np.expand_dims(img_array, axis=0)
        
        # Pr√©-processamento espec√≠fico do MobileNetV2 (normaliza√ß√£o [-1, 1])
        img_array = preprocess_input(img_array)
        
        # Remove dimens√£o do batch
        img_array = np.squeeze(img_array, axis=0)
        
        return img_array
        
    def create_data_generator(self, df_patches, batch_size=32, shuffle=True, augment=False):
        """
        Cria gerador de dados para treinamento
        Nota: batch_size maior que Inception pois MobileNet √© mais leve
        
        Args:
            df_patches: DataFrame com informa√ß√µes dos patches
            batch_size: Tamanho do batch (padr√£o 32 para MobileNet)
            shuffle: Se deve embaralhar os dados
            augment: Se deve aplicar data augmentation
            
        Yields:
            Batches de (imagens, labels)
        """
        # Configura√ß√£o de data augmentation otimizada para patches histol√≥gicos
        if augment:
            datagen = ImageDataGenerator(
                rotation_range=90,  # Rota√ß√£o em qualquer √¢ngulo
                width_shift_range=0.1,
                height_shift_range=0.1,
                horizontal_flip=True,
                vertical_flip=True,
                zoom_range=0.15,  # Um pouco mais de zoom para MobileNet
                fill_mode='nearest',
                preprocessing_function=None  # N√£o usar preprocessing aqui
            )
        else:
            datagen = ImageDataGenerator(preprocessing_function=None)
            
        while True:
            if shuffle:
                df_shuffled = df_patches.sample(frac=1).reset_index(drop=True)
            else:
                df_shuffled = df_patches
                
            for i in range(0, len(df_shuffled), batch_size):
                batch_df = df_shuffled.iloc[i:i+batch_size]
                
                batch_images = []
                batch_labels = []
                
                for _, row in batch_df.iterrows():
                    # Carrega imagem sem pr√©-processamento
                    img = load_img(row['patch_path'], target_size=self.input_size)
                    img = img_to_array(img)
                    
                    # Aplica augmentation se especificado
                    if augment:
                        img = datagen.random_transform(img)
                    
                    # Aplica pr√©-processamento do MobileNetV2
                    img = np.expand_dims(img, axis=0)
                    img = preprocess_input(img)
                    img = np.squeeze(img, axis=0)
                        
                    batch_images.append(img)
                    
                    # One-hot encoding da label
                    label = np.zeros(self.num_classes)
                    label[int(row['class'])] = 1
                    batch_labels.append(label)
                    
                yield np.array(batch_images), np.array(batch_labels)
                
    def build_model(self, fine_tune=True, freeze_layers=100):
        """
        Constr√≥i modelo MobileNetV2 pr√©-treinado para 3 classes
        MobileNetV2 tem menos camadas que Inception, ent√£o ajustamos freeze_layers
        
        Args:
            fine_tune: Se deve fazer fine-tuning das camadas
            freeze_layers: N√∫mero de camadas a congelar (do in√≠cio)
        """
        print("Construindo modelo MobileNetV2 pr√©-treinado...")
        
        # Cria modelo base pr√©-treinado (sem as camadas de classifica√ß√£o)
        base_model = MobileNetV2(
            weights='imagenet',
            include_top=False,
            input_shape=(224, 224, 3),
            alpha=1.0  # Largura padr√£o do modelo
        )
        
        # MobileNet tem menos camadas que Inception (~155 vs ~311)
        print(f"MobileNetV2 tem {len(base_model.layers)} camadas")
        
        # Adiciona camadas de classifica√ß√£o personalizada
        x = base_model.output
        x = GlobalAveragePooling2D()(x)
        
        # Camadas densas menores que Inception (MobileNet √© mais eficiente)
        x = Dense(512, activation='relu', name='fc1')(x)
        x = Dropout(0.5, name='dropout1')(x)
        x = Dense(256, activation='relu', name='fc2')(x)
        x = Dropout(0.3, name='dropout2')(x)
        
        # Camada de sa√≠da
        predictions = Dense(self.num_classes, activation='softmax', name='aln_predictions')(x)
        
        self.model = Model(inputs=base_model.input, outputs=predictions)
        
        # Configura√ß√£o de fine-tuning
        if fine_tune:
            # MobileNet tem menos camadas, ent√£o ajustamos proporcionalmente
            freeze_layers = min(freeze_layers, 100)  # No m√°ximo 100 camadas congeladas
            
            # Congela as primeiras camadas
            for layer in base_model.layers[:freeze_layers]:
                layer.trainable = False
            
            # Descongela as camadas finais
            for layer in base_model.layers[freeze_layers:]:
                layer.trainable = True
                
            print(f"Fine-tuning habilitado: {len(base_model.layers) - freeze_layers} camadas trein√°veis")
        else:
            # Congela todo o modelo base
            for layer in base_model.layers:
                layer.trainable = False
            print("Modelo base congelado - apenas camadas de classifica√ß√£o trein√°veis")
        
        # Compila modelo com otimizador apropriado
        if fine_tune:
            # Learning rate menor para fine-tuning
            optimizer = Adam(learning_rate=0.0001)
        else:
            # Learning rate normal para feature extraction
            optimizer = Adam(learning_rate=0.001)
            
        self.model.compile(
            optimizer=optimizer,
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        print("Modelo constru√≠do com sucesso!")
        print(f"Total de par√¢metros: {self.model.count_params():,}")
        
        # Conta par√¢metros trein√°veis
        trainable_count = np.sum([tf.keras.backend.count_params(w) 
                                 for w in self.model.trainable_weights])
        print(f"Par√¢metros trein√°veis: {trainable_count:,}")
        
        return self.model
        
    def calculate_class_weights(self, train_df):
        """Calcula pesos das classes para balanceamento"""
        y_train = train_df['class'].values
        classes = np.unique(y_train)
        
        weights = compute_class_weight(
            class_weight='balanced',
            classes=classes,
            y=y_train
        )
        
        self.class_weights = dict(zip(classes, weights))
        print(f"Pesos das classes: {self.class_weights}")
        
        return self.class_weights
        
    def train_model(self, datasets, epochs=30, batch_size=32, fine_tune_epochs=10):
        """
        Treina o modelo com estrat√©gia de two-stage training
        MobileNet treina mais r√°pido, ent√£o podemos usar mais √©pocas
        
        Args:
            datasets: Dicion√°rio com datasets de treino, valida√ß√£o e teste
            epochs: N√∫mero de √©pocas para feature extraction
            batch_size: Tamanho do batch (maior para MobileNet)
            fine_tune_epochs: N√∫mero de √©pocas para fine-tuning
            
        Returns:
            Hist√≥ria do treinamento
        """
        train_df = datasets['train']
        val_df = datasets['val']
        
        # Calcula pesos das classes
        self.calculate_class_weights(train_df)
        
        # Cria geradores
        train_generator = self.create_data_generator(
            train_df, 
            batch_size=batch_size, 
            shuffle=True, 
            augment=True
        )
        
        val_generator = self.create_data_generator(
            val_df, 
            batch_size=batch_size, 
            shuffle=False, 
            augment=False
        )
        
        # Callbacks otimizados para MobileNet
        callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=5,
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=3,
                min_lr=1e-7,
                verbose=1
            ),
            ModelCheckpoint(
                'mobilenet_v2_best.h5',
                monitor='val_accuracy',
                save_best_only=True,
                verbose=1
            )
        ]
        
        # Calcula steps
        steps_per_epoch = len(train_df) // batch_size
        validation_steps = len(val_df) // batch_size
        
        print(f"\nüöÄ Iniciando treinamento MobileNetV2...")
        print(f"üìä Steps por √©poca: {steps_per_epoch}")
        print(f"üìä Validation steps: {validation_steps}")
        
        # Stage 1: Feature extraction (modelo base congelado)
        if epochs > 0:
            print(f"\nüìå Stage 1: Feature Extraction ({epochs} √©pocas)")
            
            # Reconstr√≥i modelo sem fine-tuning
            self.build_model(fine_tune=False)
            
            history_fe = self.model.fit(
                train_generator,
                steps_per_epoch=steps_per_epoch,
                epochs=epochs,
                validation_data=val_generator,
                validation_steps=validation_steps,
                callbacks=callbacks,
                class_weight=self.class_weights,
                verbose=1
            )
        
        # Stage 2: Fine-tuning
        if fine_tune_epochs and fine_tune_epochs > 0:
            print(f"\nüìå Stage 2: Fine-tuning ({fine_tune_epochs} √©pocas)")
            
            # Reconstr√≥i modelo com fine-tuning
            # Carrega pesos do stage 1
            if epochs > 0:
                weights = self.model.get_weights()
                self.build_model(fine_tune=True, freeze_layers=80)  # Menos camadas para MobileNet
                self.model.set_weights(weights)
            else:
                self.build_model(fine_tune=True, freeze_layers=80)
            
            # Callbacks com learning rate ainda menor
            ft_callbacks = [
                EarlyStopping(
                    monitor='val_loss',
                    patience=5,
                    restore_best_weights=True,
                    verbose=1
                ),
                ReduceLROnPlateau(
                    monitor='val_loss',
                    factor=0.5,
                    patience=2,
                    min_lr=1e-8,
                    verbose=1
                ),
                ModelCheckpoint(
                    'mobilenet_v2_finetuned.h5',
                    monitor='val_accuracy',
                    save_best_only=True,
                    verbose=1
                )
            ]
            
            history_ft = self.model.fit(
                train_generator,
                steps_per_epoch=steps_per_epoch,
                epochs=fine_tune_epochs,
                validation_data=val_generator,
                validation_steps=validation_steps,
                callbacks=ft_callbacks,
                class_weight=self.class_weights,
                verbose=1
            )
            
            # Combina hist√≥rias se ambos stages foram executados
            if epochs > 0:
                history = {
                    'loss': history_fe.history['loss'] + history_ft.history['loss'],
                    'accuracy': history_fe.history['accuracy'] + history_ft.history['accuracy'],
                    'val_loss': history_fe.history['val_loss'] + history_ft.history['val_loss'],
                    'val_accuracy': history_fe.history['val_accuracy'] + history_ft.history['val_accuracy']
                }
            else:
                history = history_ft.history
        else:
            history = history_fe.history if epochs > 0 else None
            
        print("\n‚úÖ Treinamento conclu√≠do!")
        
        return history
        
    def evaluate_model(self, test_df, batch_size=32):
        """
        Avalia modelo no conjunto de teste
        
        Args:
            test_df: DataFrame com dados de teste
            batch_size: Tamanho do batch
            
        Returns:
            Dicion√°rio com m√©tricas de avalia√ß√£o
        """
        print("\nüìä Avaliando modelo no conjunto de teste...")
        
        # Cria gerador sem augmentation
        test_generator = self.create_data_generator(
            test_df,
            batch_size=batch_size,
            shuffle=False,
            augment=False
        )
        
        # Predi√ß√µes
        steps = len(test_df) // batch_size + (1 if len(test_df) % batch_size else 0)
        
        y_true = []
        y_pred = []
        
        for i in range(steps):
            batch_x, batch_y = next(test_generator)
            predictions = self.model.predict(batch_x, verbose=0)
            
            y_true.extend(np.argmax(batch_y, axis=1))
            y_pred.extend(np.argmax(predictions, axis=1))
            
            # Para garantir que n√£o pegamos dados extras
            if len(y_true) >= len(test_df):
                y_true = y_true[:len(test_df)]
                y_pred = y_pred[:len(test_df)]
                break
        
        # M√©tricas
        accuracy = np.mean(np.array(y_true) == np.array(y_pred))
        cm = confusion_matrix(y_true, y_pred)
        report = classification_report(
            y_true, 
            y_pred, 
            target_names=self.class_names,
            output_dict=True
        )
        
        print(f"\n‚úÖ Acur√°cia no teste: {accuracy:.4f}")
        print("\nüìã Relat√≥rio de classifica√ß√£o:")
        print(classification_report(
            y_true, 
            y_pred, 
            target_names=self.class_names
        ))
        
        return {
            'accuracy': accuracy,
            'confusion_matrix': cm,
            'classification_report': report,
            'y_true': y_true,
            'y_pred': y_pred
        }
        
    def predict(self, image_path):
        """
        Faz predi√ß√£o para uma √∫nica imagem
        
        Args:
            image_path: Caminho da imagem
            
        Returns:
            Classe predita e probabilidades
        """
        # Pr√©-processa imagem
        img = self.preprocess_image(image_path)
        img = np.expand_dims(img, axis=0)
        
        # Predi√ß√£o
        predictions = self.model.predict(img, verbose=0)
        predicted_class = np.argmax(predictions[0])
        
        return {
            'class': self.class_names[predicted_class],
            'class_idx': predicted_class,
            'probabilities': predictions[0],
            'confidence': float(predictions[0][predicted_class])
        }
        
    def save_model(self, filepath):
        """Salva modelo treinado"""
        self.model.save(filepath)
        print(f"‚úÖ Modelo salvo em: {filepath}")
        
    def load_model(self, filepath):
        """Carrega modelo salvo"""
        from tensorflow.keras.models import load_model
        self.model = load_model(filepath)
        print(f"‚úÖ Modelo carregado de: {filepath}")
        
    def plot_training_history(self, history):
        """
        Plota hist√≥rico de treinamento
        
        Args:
            history: Hist√≥rico retornado pelo fit
        """
        plt.figure(figsize=(12, 4))
        
        # Accuracy
        plt.subplot(1, 2, 1)
        plt.plot(history['accuracy'], label='Treino')
        plt.plot(history['val_accuracy'], label='Valida√ß√£o')
        plt.title('Acur√°cia do Modelo MobileNetV2')
        plt.xlabel('√âpoca')
        plt.ylabel('Acur√°cia')
        plt.legend()
        plt.grid(True)
        
        # Loss
        plt.subplot(1, 2, 2)
        plt.plot(history['loss'], label='Treino')
        plt.plot(history['val_loss'], label='Valida√ß√£o')
        plt.title('Loss do Modelo MobileNetV2')
        plt.xlabel('√âpoca')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        plt.tight_layout()
        plt.savefig('mobilenet_training_history.png', dpi=300, bbox_inches='tight')
        plt.show()
        
    def plot_confusion_matrix(self, cm):
        """
        Plota matriz de confus√£o
        
        Args:
            cm: Matriz de confus√£o
        """
        plt.figure(figsize=(8, 6))
        sns.heatmap(
            cm, 
            annot=True, 
            fmt='d', 
            cmap='Blues',
            xticklabels=self.class_names,
            yticklabels=self.class_names
        )
        plt.title('Matriz de Confus√£o - MobileNetV2')
        plt.xlabel('Predito')
        plt.ylabel('Verdadeiro')
        plt.tight_layout()
        plt.savefig('mobilenet_confusion_matrix.png', dpi=300, bbox_inches='tight')
        plt.show()
